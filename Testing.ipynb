{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "class EKFAC(torch.optim.Optimizer):\n",
    "    \"\"\" Implements the Eigenvalue-corrected Kronecker-factored Optimized Curvature preconditioner \n",
    "    \n",
    "    See details at https://arxiv.org/pdf/1806.03884.pdf\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 network, \n",
    "                 recompute_KFAC_steps=1,\n",
    "                 epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            network - the network to operate on\n",
    "            recompute_KFAC_steps (integer) - the number of steps between successive recomputations of the \n",
    "                                             Kronecker factors of the layer-wise Fisher matrix\n",
    "            epsilon (float) - the damping parameter used to avoid infinities\"\"\"\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.params_by_layer = []\n",
    "        \n",
    "        self.modules_with_weights = [torch.nn.Bilinear, \n",
    "                        torch.nn.Conv1d,\n",
    "                        torch.nn.Conv2d,\n",
    "                        torch.nn.Conv3d,\n",
    "                        torch.nn.ConvTranspose1d,\n",
    "                        torch.nn.ConvTranspose2d,\n",
    "                        torch.nn.ConvTranspose3d,\n",
    "                        torch.nn.Linear,                        \n",
    "                       ]\n",
    "        \n",
    "        self.stored_items = {}\n",
    "        \n",
    "        # need to keep track of iteration because we only recompute KFAC matrices every 'self.recompute_KFAC_steps' steps \n",
    "        self.iteration_number = 0 \n",
    "        self.recompute_KFAC_steps = recompute_KFAC_steps\n",
    "        \n",
    "        tracked_modules_count = 0\n",
    "        for layer in network.modules():\n",
    "            if type(layer) in self.modules_with_weights: \n",
    "                if type(layer) != torch.nn.Linear:\n",
    "                    warnings.warn('Have not tested this for any module type other than linear')\n",
    "                    \n",
    "                # add functions to the module such that for all layers with weights\n",
    "                layer.register_forward_pre_hook(self.store_input)\n",
    "                layer.register_backward_hook(self.store_grad_output)\n",
    "                \n",
    "                # add parameters to the list, grouped by layer \n",
    "                self.params_by_layer.append({'params': [layer.weight]})\n",
    "                if layer.bias is not None:\n",
    "                    self.params_by_layer[-1]['params'].append(layer.bias)\n",
    "        \n",
    "                # make a label for the module and add it to the keys of the stored_items dictionary\n",
    "                tracked_modules_count += 1\n",
    "                self.stored_items[layer] = {} \n",
    "                \n",
    "        default_options = {}\n",
    "        super(EKFAC, self).__init__(self.params_by_layer, default_options)\n",
    "       \n",
    "    def step(self):\n",
    "        \n",
    "        if self.iteration_number % self.recompute_KFAC_steps == 0:\n",
    "            self.compute_Kronecker_matrices()\n",
    "            \n",
    "        self.compute_scalings()\n",
    "#         self.precondition()\n",
    "        \n",
    "        self.iteration_number += 1\n",
    "        \n",
    "    def store_input(self, module, inputs_to_module):\n",
    "        \"\"\" When called before running each layer with weights, this function stores\n",
    "        the input to the layer\"\"\"\n",
    "        \n",
    "        self.stored_items[module]['input'] = inputs_to_module[0]\n",
    "    \n",
    "    def store_grad_output(self, module, grad_wrt_input, grad_wrt_output):\n",
    "        \"\"\" When called after the backward pass of each layer with weights, this function\n",
    "        stores the gradient of the backwards-running function (usually the loss function) with respect\n",
    "        to the pre-activations, i.e. the output of the layer\"\"\"\n",
    "        \n",
    "        # We have to scale by the batch size, because the grad_wrt_output which is passed to the \n",
    "        # function is already scaled down by batch_size, even though we did not do any reduction\n",
    "        self.stored_items[module]['grad_wrt_output'] = grad_wrt_output[0] * grad_wrt_output[0].size(0) \n",
    "        \n",
    "    def compute_Kronecker_matrices(self):\n",
    "        \"\"\" For each layer (or, more properly, parameter group), computes the Kronecker-factored matrices, \n",
    "        where the Kronecker factors are defined by \n",
    "        A = E[input_to_layer @ input_to_layer.T]\n",
    "        B = E[grad_wrt_output @ grad_wrt_output.T]\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            # notation follows the EKFAC paper\n",
    "            h = stored_values['input'].t()\n",
    "            delta = stored_values['grad_wrt_output']\n",
    "            \n",
    "            # We want E[ h @ h.T]\n",
    "            # h should always be of size (n_inputs, batch_size)\n",
    "            # delta should be of size (batch_size, n_outputs)\n",
    "            with torch.no_grad():\n",
    "                A = h @ h.transpose(1,0) / h.shape[1]\n",
    "                B = delta.transpose(1,0) @ delta / delta.shape[0]\n",
    "            \n",
    "            # Eigendecompose A and B to get UA and UB, which contain the eigenvectors\n",
    "            # UA @ diag(EvalsA) @ UA.t() = A\n",
    "            EvalsA, UA = torch.symeig(A, eigenvectors=True)\n",
    "            EvalsB, UB = torch.symeig(B, eigenvectors=True)\n",
    "            \n",
    "            self.stored_items[layer]['UA'] = UA\n",
    "            self.stored_items[layer]['UB'] = UB\n",
    "            \n",
    "    def compute_scalings(self):\n",
    "        \n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            UA = stored_values['UA']\n",
    "            UB = stored_values['UB']\n",
    "            h = stored_values['input'].t()\n",
    "            delta = stored_values['grad_wrt_output']\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_size = h.shape[1]\n",
    "                # TODO Check that this is correct\n",
    "                # Because delta and h contain information for each training example in the mini-batch,\n",
    "                # when we do the matrix multiplication in the middle, we are averaging over the mini-batch.\n",
    "                # So, we need to square the values first, so we can square-then-average, not average-then-square.\n",
    "                scalings = ((UB.t() @ delta.t())**2) @ ((h.t() @ UA)**2) / batch_size\n",
    "                \n",
    "            stored_values['scalings'] = scalings\n",
    "            \n",
    "    def precondition(self):\n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            \n",
    "            UA = stored_values['UA']\n",
    "            UB = stored_values['UB']\n",
    "            \n",
    "            S = stored_values['scalings']\n",
    "            \n",
    "            grad_mb = layer.weight.grad.data # mb stands for 'mini-batch'\n",
    "            grad_mb_kfe = UB @ grad_mb @ UA.t()\n",
    "            grad_mb_kfe_scaled = grad_mb_kfe / (S + self.epsilon)\n",
    "            grad_mb_orig = UB.t() @ grad_mb @ UA # back to original basis \n",
    "            \n",
    "            layer.weight.grad.data = grad_mb_orig\n",
    "            \n",
    "    def approximate_Fisher_matrix(self, to_return=False):\n",
    "        \"\"\" For testing/debugging, compute the layer-wise approximation to the empirical Fisher matrix \n",
    "            to compare to the Fischer information matrix \"\"\"\n",
    "        approximate_Fisher_matrices = []\n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            \n",
    "            UA = stored_values['UA'].numpy()\n",
    "            UB = stored_values['UB'].numpy()\n",
    "            S = np.diag(stored_values['scalings'].numpy().reshape(-1))\n",
    "            \n",
    "            UAkronUB = np.kron(UA, UB)\n",
    "            \n",
    "            approximate_Fisher = UAkronUB @ S @ UAkronUB.T\n",
    "            approximate_Fisher_matrices.append(approximate_Fisher)\n",
    "            \n",
    "            stored_values['aproximate_Fisher'] = torch.tensor(approximate_Fisher)\n",
    "        \n",
    "        if to_return:\n",
    "            return approximate_Fisher_matrices\n",
    "            \n",
    "    def compute_hdeltaT(self):\n",
    "        \"\"\" For testing/debugging, compute the layer-wise h delta T product.\n",
    "        The minibatch-averaged h delta^T product should be equal to the gradient of the \n",
    "        weigt matrix for each linear layer.\"\"\"\n",
    "        \n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            h = stored_values['input']\n",
    "            delta = stored_values['grad_wrt_output']\n",
    "            stored_values['hdeltaT'] = h.t() @ delta / h.size(0)\n",
    "            \n",
    "    def compute_empirical_Fisher_matrix(self, to_return=False):\n",
    "        \"\"\" For testing/debugging, compute empirical Fisher matrix \"\"\"\n",
    "        \n",
    "        empirical_fisher_matrices = []\n",
    "        for layer, stored_values in self.stored_items.items():\n",
    "            h = stored_values['input']\n",
    "            delta = stored_values['grad_wrt_output']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                empirical_fisher_matrix = empirical_fisher(h, delta)    \n",
    "                stored_values['empirical_fisher'] = empirical_fisher_matrix\n",
    "                empirical_fisher_matrices.append(empirical_fisher_matrix)\n",
    "        \n",
    "        if to_return:\n",
    "            return empirical_fisher_matrices\n",
    "        \n",
    "def outer_prod_individual(M1, M2):\n",
    "    \"\"\" takes the outer product of M1 and M2, where M1 is NxA, and M2 is NxB\n",
    "    \"\"\"\n",
    "    return torch.einsum('ij,ik->ijk', M1, M2)\n",
    "\n",
    "def vectorize_individual(M):\n",
    "    \"\"\" Given a tensor M, with size (A, B, C), vectorizes this tensor, leaving the first dimension intact,\n",
    "    by stacking columns, resulting in a tensor of size (A, BC)\"\"\"\n",
    "    Mt = M.transpose(1,2)\n",
    "    return Mt.contiguous().view(Mt.size(0), -1)\n",
    "\n",
    "def empirical_fisher(h, delta):\n",
    "    \"\"\" given h, representing the input to a layer, and delta, representing the gradient with respect to its output,\n",
    "    computes the empirical fisher matrix, averaged over the minibatch\n",
    "    \n",
    "    Arguments:\n",
    "    h - torch.tensor, dimension (batch_size) * (n_inputs)\n",
    "    delta - torch.tensor, dimension (batch_size) * (n_outputs)\"\"\"\n",
    "    \n",
    "    grad_individual = outer_prod_individual(h, delta)\n",
    "    vec_grad_individual = vectorize_individual(grad_individual)\n",
    "    fisher_individual = outer_prod_individual(vec_grad_individual, vec_grad_individual)\n",
    "    return fisher_individual.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "Nbatch, D_in, H, D_out = 20, 2, 4, 1\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "EKFAC_one = EKFAC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(Nbatch, D_in)\n",
    "y = torch.randn(Nbatch, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = model(x)\n",
    "loss_fun = torch.nn.MSELoss(reduction='sum')\n",
    "z = loss_fun(y, y_mod)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "EKFAC_one.step()\n",
    "EKFAC_one.compute_hdeltaT()\n",
    "EKFAC_one.compute_empirical_Fisher_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for testing empirical Fisher matrix implementation\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "Nbatch, D_in, H, D_out = 1, 2, 9, 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(H, D_out, bias=False)\n",
    ")\n",
    "\n",
    "EKFAC_one = EKFAC(model)\n",
    "\n",
    "x = torch.randn(Nbatch, D_in)\n",
    "y = torch.randn(Nbatch, D_out)\n",
    "\n",
    "# compute Fisher matrix altogether\n",
    "y_mod = model(x)\n",
    "loss_fun = torch.nn.MSELoss(reduction='mean')\n",
    "z = loss_fun(y, y_mod)\n",
    "z.backward()\n",
    "\n",
    "empirical_Fisher_matrix = EKFAC_one.compute_empirical_Fisher_matrix(to_return=True)\n",
    "EKFAC_one.step()\n",
    "approximate_Fisher_matrix = EKFAC_one.approximate_Fisher_matrix(to_return=True)\n",
    "\n",
    "# L1grads = []\n",
    "# L2grads = []\n",
    "\n",
    "# for i in range(Nbatch):\n",
    "#     EKFAC_one.zero_grad()\n",
    "    \n",
    "#     x_test = x[i]\n",
    "#     y_test = y[i]\n",
    "    \n",
    "#     y_mod = model(x_test)\n",
    "#     z = loss_fun(y_test, y_mod)\n",
    "#     z.backward()\n",
    "    \n",
    "#     for ind, module in enumerate(model.modules()):\n",
    "#         if ind == 1:\n",
    "#             L1grads.append(module.weight.grad.clone().numpy())\n",
    "#         elif ind == 3:\n",
    "#             L2grads.append(module.weight.grad.clone().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03250405, -0.17115346, -0.12354335, -0.10530421, -0.16963662,\n",
       "        -0.2080802 , -0.22366273, -0.15949368, -0.15731049],\n",
       "       [-0.17115346, -0.04094686, -0.0724525 , -0.18210094, -0.38260227,\n",
       "        -0.37862158, -0.26161712, -0.2489062 , -0.20018882],\n",
       "       [-0.12354335, -0.07245251, -0.00435159, -0.19718952, -0.09267814,\n",
       "        -0.11479546, -0.16185072, -0.13504566, -0.13439783],\n",
       "       [-0.10530421, -0.18210094, -0.19718952,  0.07043797, -0.51852846,\n",
       "        -0.44541976, -0.28361455, -0.27673942, -0.19396444],\n",
       "       [-0.16963664, -0.38260227, -0.09267814, -0.51852846,  0.14828075,\n",
       "        -0.02879338, -0.29201096, -0.28335407, -0.24485016],\n",
       "       [-0.2080802 , -0.37862158, -0.11479546, -0.44541976, -0.02879336,\n",
       "        -0.04450651, -0.22582628, -0.21239723, -0.30395103],\n",
       "       [-0.22366273, -0.26161712, -0.16185072, -0.28361455, -0.292011  ,\n",
       "        -0.22582628, -0.03130716, -0.25572693, -0.19974801],\n",
       "       [-0.15949368, -0.24890621, -0.13504566, -0.27673942, -0.28335407,\n",
       "        -0.21239723, -0.25572693, -0.13727388, -0.30729592],\n",
       "       [-0.15731049, -0.20018882, -0.13439783, -0.19396445, -0.24485016,\n",
       "        -0.30395103, -0.19974801, -0.30729592,  0.07217185]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate_Fisher_matrix[1] - empirical_Fisher_matrix[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8731704, 1.9741668, 2.728336 , 2.3566275, 2.5245583, 1.8503456,\n",
       "        2.7413404, 2.5156274, 2.0007644],\n",
       "       [1.9741668, 1.3564578, 1.8746506, 1.6192482, 1.734634 , 1.2713798,\n",
       "        1.8835859, 1.7284976, 1.3747332],\n",
       "       [2.728336 , 1.8746506, 2.5908027, 2.2378318, 2.3972974, 1.7570713,\n",
       "        2.6031516, 2.3888166, 1.8999074],\n",
       "       [2.3566275, 1.6192482, 2.2378318, 1.9329494, 2.0706894, 1.5176878,\n",
       "        2.248498 , 2.0633643, 1.6410639],\n",
       "       [2.5245583, 1.734634 , 2.3972974, 2.0706894, 2.2182446, 1.6258366,\n",
       "        2.4087238, 2.2103975, 1.7580044],\n",
       "       [1.8503456, 1.2713798, 1.7570713, 1.5176878, 1.6258366, 1.1916381,\n",
       "        1.7654461, 1.6200851, 1.288509 ],\n",
       "       [2.7413404, 1.8835859, 2.6031516, 2.248498 , 2.4087238, 1.7654461,\n",
       "        2.615559 , 2.4002028, 1.9089631],\n",
       "       [2.5156274, 1.7284976, 2.3888166, 2.0633643, 2.2103975, 1.6200851,\n",
       "        2.4002028, 2.2025778, 1.7517854],\n",
       "       [2.0007644, 1.3747332, 1.8999074, 1.6410639, 1.7580044, 1.288509 ,\n",
       "        1.9089631, 1.7517854, 1.3932548]], dtype=float32)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Linear(in_features=2, out_features=9, bias=False): {'UA': tensor([[-0.0587, -0.9983],\n",
       "          [-0.9983,  0.0587]]),\n",
       "  'UB': tensor([[ 4.8361e-01,  2.2956e-01, -1.6497e-03, -7.2406e-02, -3.5454e-02,\n",
       "            5.7071e-01, -7.2590e-02,  4.4047e-01,  4.2653e-01],\n",
       "          [-7.1136e-02, -3.8768e-01, -3.4171e-01, -8.0973e-01,  1.2047e-01,\n",
       "            2.1952e-01,  9.8885e-03, -4.3114e-02, -8.6967e-02],\n",
       "          [-6.7117e-01, -3.8924e-01,  1.1366e-01,  2.5546e-01,  9.1139e-03,\n",
       "            3.8695e-01, -5.1582e-02,  8.5658e-02,  4.0005e-01],\n",
       "          [ 6.9080e-02, -5.5708e-02, -8.7970e-01,  4.3574e-01, -2.1809e-02,\n",
       "            1.2946e-01,  1.0069e-02, -7.4586e-02, -7.4077e-02],\n",
       "          [ 4.1001e-01, -5.4350e-01,  2.4375e-01,  2.8049e-01,  5.4681e-01,\n",
       "            1.7453e-01,  3.1730e-02, -4.8832e-02, -2.5604e-01],\n",
       "          [ 2.4913e-01, -4.7546e-01, -1.0046e-01, -5.3937e-03, -1.5684e-01,\n",
       "           -5.9822e-01, -5.1746e-02,  2.2947e-01,  5.1375e-01],\n",
       "          [-1.7178e-01,  3.4700e-01, -1.5403e-01, -6.6373e-02,  8.1026e-01,\n",
       "           -2.3247e-01, -2.3234e-02,  3.8637e-02,  3.3070e-01],\n",
       "          [ 2.1373e-01,  2.2939e-02,  5.6644e-02, -2.9115e-02, -5.9405e-02,\n",
       "            1.3117e-01,  8.2835e-02, -8.4822e-01,  4.5018e-01],\n",
       "          [-2.1528e-02, -2.1753e-04, -3.2547e-03,  3.2627e-03, -4.8416e-03,\n",
       "            5.2029e-03,  9.9035e-01,  1.2334e-01,  5.8873e-02]]),\n",
       "  'aproximate_Fisher': tensor([[ 4.4995e-03, -9.1742e-04,  4.2201e-03, -7.8144e-04, -2.7010e-03,\n",
       "            5.4195e-03,  3.4885e-03,  4.7489e-03,  6.2106e-04, -2.6440e-04,\n",
       "            5.3909e-05, -2.4798e-04,  4.5919e-05,  1.5872e-04, -3.1846e-04,\n",
       "           -2.0499e-04, -2.7906e-04, -3.6495e-05],\n",
       "          [-9.1742e-04,  1.8706e-04, -8.6045e-04,  1.5933e-04,  5.5071e-04,\n",
       "           -1.1050e-03, -7.1129e-04, -9.6827e-04, -1.2663e-04,  5.3909e-05,\n",
       "           -1.0992e-05,  5.0562e-05, -9.3625e-06, -3.2361e-05,  6.4932e-05,\n",
       "            4.1797e-05,  5.6898e-05,  7.4410e-06],\n",
       "          [ 4.2201e-03, -8.6045e-04,  3.9580e-03, -7.3291e-04, -2.5333e-03,\n",
       "            5.0830e-03,  3.2719e-03,  4.4540e-03,  5.8249e-04, -2.4798e-04,\n",
       "            5.0562e-05, -2.3258e-04,  4.3067e-05,  1.4886e-04, -2.9869e-04,\n",
       "           -1.9226e-04, -2.6173e-04, -3.4228e-05],\n",
       "          [-7.8144e-04,  1.5933e-04, -7.3291e-04,  1.3571e-04,  4.6908e-04,\n",
       "           -9.4122e-04, -6.0586e-04, -8.2475e-04, -1.0786e-04,  4.5919e-05,\n",
       "           -9.3625e-06,  4.3067e-05, -7.9748e-06, -2.7564e-05,  5.5308e-05,\n",
       "            3.5602e-05,  4.8464e-05,  6.3381e-06],\n",
       "          [-2.7010e-03,  5.5071e-04, -2.5333e-03,  4.6908e-04,  1.6214e-03,\n",
       "           -3.2533e-03, -2.0941e-03, -2.8507e-03, -3.7281e-04,  1.5872e-04,\n",
       "           -3.2361e-05,  1.4886e-04, -2.7564e-05, -9.5274e-05,  1.9117e-04,\n",
       "            1.2305e-04,  1.6751e-04,  2.1907e-05],\n",
       "          [ 5.4195e-03, -1.1050e-03,  5.0830e-03, -9.4122e-04, -3.2533e-03,\n",
       "            6.5277e-03,  4.2019e-03,  5.7199e-03,  7.4805e-04, -3.1846e-04,\n",
       "            6.4932e-05, -2.9869e-04,  5.5308e-05,  1.9117e-04, -3.8358e-04,\n",
       "           -2.4691e-04, -3.3612e-04, -4.3957e-05],\n",
       "          [ 3.4885e-03, -7.1129e-04,  3.2719e-03, -6.0586e-04, -2.0941e-03,\n",
       "            4.2019e-03,  2.7047e-03,  3.6819e-03,  4.8152e-04, -2.0499e-04,\n",
       "            4.1797e-05, -1.9226e-04,  3.5602e-05,  1.2305e-04, -2.4691e-04,\n",
       "           -1.5894e-04, -2.1636e-04, -2.8295e-05],\n",
       "          [ 4.7489e-03, -9.6827e-04,  4.4540e-03, -8.2475e-04, -2.8507e-03,\n",
       "            5.7199e-03,  3.6819e-03,  5.0121e-03,  6.5548e-04, -2.7906e-04,\n",
       "            5.6898e-05, -2.6173e-04,  4.8464e-05,  1.6751e-04, -3.3612e-04,\n",
       "           -2.1636e-04, -2.9452e-04, -3.8517e-05],\n",
       "          [ 6.2106e-04, -1.2663e-04,  5.8249e-04, -1.0786e-04, -3.7281e-04,\n",
       "            7.4805e-04,  4.8152e-04,  6.5548e-04,  8.5723e-05, -3.6495e-05,\n",
       "            7.4410e-06, -3.4228e-05,  6.3381e-06,  2.1907e-05, -4.3957e-05,\n",
       "           -2.8295e-05, -3.8517e-05, -5.0373e-06],\n",
       "          [-2.6440e-04,  5.3909e-05, -2.4798e-04,  4.5919e-05,  1.5872e-04,\n",
       "           -3.1846e-04, -2.0499e-04, -2.7906e-04, -3.6495e-05,  1.5537e-05,\n",
       "           -3.1678e-06,  1.4572e-05, -2.6983e-06, -9.3264e-06,  1.8714e-05,\n",
       "            1.2046e-05,  1.6398e-05,  2.1445e-06],\n",
       "          [ 5.3909e-05, -1.0992e-05,  5.0562e-05, -9.3625e-06, -3.2361e-05,\n",
       "            6.4932e-05,  4.1797e-05,  5.6898e-05,  7.4410e-06, -3.1678e-06,\n",
       "            6.4590e-07, -2.9711e-06,  5.5016e-07,  1.9016e-06, -3.8156e-06,\n",
       "           -2.4561e-06, -3.3434e-06, -4.3725e-07],\n",
       "          [-2.4798e-04,  5.0562e-05, -2.3258e-04,  4.3067e-05,  1.4886e-04,\n",
       "           -2.9869e-04, -1.9226e-04, -2.6173e-04, -3.4228e-05,  1.4572e-05,\n",
       "           -2.9711e-06,  1.3667e-05, -2.5307e-06, -8.7473e-06,  1.7551e-05,\n",
       "            1.1298e-05,  1.5380e-05,  2.0113e-06],\n",
       "          [ 4.5919e-05, -9.3625e-06,  4.3067e-05, -7.9748e-06, -2.7564e-05,\n",
       "            5.5308e-05,  3.5602e-05,  4.8464e-05,  6.3381e-06, -2.6983e-06,\n",
       "            5.5016e-07, -2.5307e-06,  4.6861e-07,  1.6197e-06, -3.2500e-06,\n",
       "           -2.0920e-06, -2.8478e-06, -3.7244e-07],\n",
       "          [ 1.5872e-04, -3.2361e-05,  1.4886e-04, -2.7564e-05, -9.5274e-05,\n",
       "            1.9117e-04,  1.2305e-04,  1.6751e-04,  2.1907e-05, -9.3264e-06,\n",
       "            1.9016e-06, -8.7473e-06,  1.6197e-06,  5.5985e-06, -1.1233e-05,\n",
       "           -7.2309e-06, -9.8434e-06, -1.2873e-06],\n",
       "          [-3.1846e-04,  6.4932e-05, -2.9869e-04,  5.5308e-05,  1.9117e-04,\n",
       "           -3.8358e-04, -2.4691e-04, -3.3612e-04, -4.3957e-05,  1.8714e-05,\n",
       "           -3.8156e-06,  1.7551e-05, -3.2500e-06, -1.1233e-05,  2.2540e-05,\n",
       "            1.4509e-05,  1.9751e-05,  2.5830e-06],\n",
       "          [-2.0499e-04,  4.1797e-05, -1.9226e-04,  3.5602e-05,  1.2305e-04,\n",
       "           -2.4691e-04, -1.5894e-04, -2.1636e-04, -2.8295e-05,  1.2046e-05,\n",
       "           -2.4561e-06,  1.1298e-05, -2.0920e-06, -7.2309e-06,  1.4509e-05,\n",
       "            9.3394e-06,  1.2714e-05,  1.6627e-06],\n",
       "          [-2.7906e-04,  5.6898e-05, -2.6173e-04,  4.8464e-05,  1.6751e-04,\n",
       "           -3.3612e-04, -2.1636e-04, -2.9452e-04, -3.8517e-05,  1.6398e-05,\n",
       "           -3.3434e-06,  1.5380e-05, -2.8478e-06, -9.8434e-06,  1.9751e-05,\n",
       "            1.2714e-05,  1.7307e-05,  2.2634e-06],\n",
       "          [-3.6495e-05,  7.4410e-06, -3.4228e-05,  6.3381e-06,  2.1907e-05,\n",
       "           -4.3957e-05, -2.8295e-05, -3.8517e-05, -5.0373e-06,  2.1445e-06,\n",
       "           -4.3725e-07,  2.0113e-06, -3.7244e-07, -1.2873e-06,  2.5830e-06,\n",
       "            1.6627e-06,  2.2634e-06,  2.9600e-07]]),\n",
       "  'empirical_fisher': tensor([[ 4.4995e-03, -2.6440e-04, -9.1742e-04,  5.3909e-05,  4.2201e-03,\n",
       "           -2.4798e-04, -7.8144e-04,  4.5919e-05, -2.7010e-03,  1.5872e-04,\n",
       "            5.4195e-03, -3.1846e-04,  3.4885e-03, -2.0499e-04,  4.7489e-03,\n",
       "           -2.7906e-04,  6.2106e-04, -3.6495e-05],\n",
       "          [-2.6440e-04,  1.5537e-05,  5.3909e-05, -3.1678e-06, -2.4798e-04,\n",
       "            1.4572e-05,  4.5919e-05, -2.6983e-06,  1.5872e-04, -9.3264e-06,\n",
       "           -3.1846e-04,  1.8714e-05, -2.0499e-04,  1.2046e-05, -2.7906e-04,\n",
       "            1.6398e-05, -3.6495e-05,  2.1445e-06],\n",
       "          [-9.1742e-04,  5.3909e-05,  1.8706e-04, -1.0992e-05, -8.6045e-04,\n",
       "            5.0562e-05,  1.5933e-04, -9.3625e-06,  5.5071e-04, -3.2361e-05,\n",
       "           -1.1050e-03,  6.4932e-05, -7.1129e-04,  4.1797e-05, -9.6827e-04,\n",
       "            5.6898e-05, -1.2663e-04,  7.4410e-06],\n",
       "          [ 5.3909e-05, -3.1678e-06, -1.0992e-05,  6.4590e-07,  5.0562e-05,\n",
       "           -2.9711e-06, -9.3625e-06,  5.5016e-07, -3.2361e-05,  1.9016e-06,\n",
       "            6.4932e-05, -3.8156e-06,  4.1797e-05, -2.4561e-06,  5.6898e-05,\n",
       "           -3.3434e-06,  7.4410e-06, -4.3725e-07],\n",
       "          [ 4.2201e-03, -2.4798e-04, -8.6045e-04,  5.0562e-05,  3.9580e-03,\n",
       "           -2.3258e-04, -7.3291e-04,  4.3067e-05, -2.5333e-03,  1.4886e-04,\n",
       "            5.0830e-03, -2.9869e-04,  3.2719e-03, -1.9226e-04,  4.4540e-03,\n",
       "           -2.6173e-04,  5.8249e-04, -3.4228e-05],\n",
       "          [-2.4798e-04,  1.4572e-05,  5.0562e-05, -2.9711e-06, -2.3258e-04,\n",
       "            1.3667e-05,  4.3067e-05, -2.5307e-06,  1.4886e-04, -8.7473e-06,\n",
       "           -2.9869e-04,  1.7551e-05, -1.9226e-04,  1.1298e-05, -2.6173e-04,\n",
       "            1.5380e-05, -3.4228e-05,  2.0113e-06],\n",
       "          [-7.8144e-04,  4.5919e-05,  1.5933e-04, -9.3625e-06, -7.3291e-04,\n",
       "            4.3067e-05,  1.3571e-04, -7.9748e-06,  4.6908e-04, -2.7564e-05,\n",
       "           -9.4122e-04,  5.5308e-05, -6.0586e-04,  3.5602e-05, -8.2475e-04,\n",
       "            4.8464e-05, -1.0786e-04,  6.3381e-06],\n",
       "          [ 4.5919e-05, -2.6983e-06, -9.3625e-06,  5.5016e-07,  4.3067e-05,\n",
       "           -2.5307e-06, -7.9748e-06,  4.6861e-07, -2.7564e-05,  1.6197e-06,\n",
       "            5.5308e-05, -3.2500e-06,  3.5602e-05, -2.0920e-06,  4.8464e-05,\n",
       "           -2.8478e-06,  6.3381e-06, -3.7244e-07],\n",
       "          [-2.7010e-03,  1.5872e-04,  5.5071e-04, -3.2361e-05, -2.5333e-03,\n",
       "            1.4886e-04,  4.6908e-04, -2.7564e-05,  1.6214e-03, -9.5274e-05,\n",
       "           -3.2533e-03,  1.9117e-04, -2.0941e-03,  1.2305e-04, -2.8507e-03,\n",
       "            1.6751e-04, -3.7281e-04,  2.1907e-05],\n",
       "          [ 1.5872e-04, -9.3264e-06, -3.2361e-05,  1.9016e-06,  1.4886e-04,\n",
       "           -8.7473e-06, -2.7564e-05,  1.6197e-06, -9.5274e-05,  5.5985e-06,\n",
       "            1.9117e-04, -1.1233e-05,  1.2305e-04, -7.2309e-06,  1.6751e-04,\n",
       "           -9.8434e-06,  2.1907e-05, -1.2873e-06],\n",
       "          [ 5.4195e-03, -3.1846e-04, -1.1050e-03,  6.4932e-05,  5.0830e-03,\n",
       "           -2.9869e-04, -9.4122e-04,  5.5308e-05, -3.2533e-03,  1.9117e-04,\n",
       "            6.5277e-03, -3.8358e-04,  4.2019e-03, -2.4691e-04,  5.7199e-03,\n",
       "           -3.3612e-04,  7.4805e-04, -4.3957e-05],\n",
       "          [-3.1846e-04,  1.8714e-05,  6.4932e-05, -3.8156e-06, -2.9869e-04,\n",
       "            1.7551e-05,  5.5308e-05, -3.2500e-06,  1.9117e-04, -1.1233e-05,\n",
       "           -3.8358e-04,  2.2540e-05, -2.4691e-04,  1.4509e-05, -3.3612e-04,\n",
       "            1.9751e-05, -4.3957e-05,  2.5830e-06],\n",
       "          [ 3.4885e-03, -2.0499e-04, -7.1129e-04,  4.1797e-05,  3.2719e-03,\n",
       "           -1.9226e-04, -6.0586e-04,  3.5602e-05, -2.0941e-03,  1.2305e-04,\n",
       "            4.2019e-03, -2.4691e-04,  2.7047e-03, -1.5894e-04,  3.6819e-03,\n",
       "           -2.1636e-04,  4.8152e-04, -2.8295e-05],\n",
       "          [-2.0499e-04,  1.2046e-05,  4.1797e-05, -2.4561e-06, -1.9226e-04,\n",
       "            1.1298e-05,  3.5602e-05, -2.0920e-06,  1.2305e-04, -7.2309e-06,\n",
       "           -2.4691e-04,  1.4509e-05, -1.5894e-04,  9.3394e-06, -2.1636e-04,\n",
       "            1.2714e-05, -2.8295e-05,  1.6627e-06],\n",
       "          [ 4.7489e-03, -2.7906e-04, -9.6827e-04,  5.6898e-05,  4.4540e-03,\n",
       "           -2.6173e-04, -8.2475e-04,  4.8464e-05, -2.8507e-03,  1.6751e-04,\n",
       "            5.7199e-03, -3.3612e-04,  3.6819e-03, -2.1636e-04,  5.0121e-03,\n",
       "           -2.9452e-04,  6.5548e-04, -3.8517e-05],\n",
       "          [-2.7906e-04,  1.6398e-05,  5.6898e-05, -3.3434e-06, -2.6173e-04,\n",
       "            1.5380e-05,  4.8464e-05, -2.8478e-06,  1.6751e-04, -9.8434e-06,\n",
       "           -3.3612e-04,  1.9751e-05, -2.1636e-04,  1.2714e-05, -2.9452e-04,\n",
       "            1.7307e-05, -3.8517e-05,  2.2634e-06],\n",
       "          [ 6.2106e-04, -3.6495e-05, -1.2663e-04,  7.4410e-06,  5.8249e-04,\n",
       "           -3.4228e-05, -1.0786e-04,  6.3381e-06, -3.7281e-04,  2.1907e-05,\n",
       "            7.4805e-04, -4.3957e-05,  4.8152e-04, -2.8295e-05,  6.5548e-04,\n",
       "           -3.8517e-05,  8.5723e-05, -5.0373e-06],\n",
       "          [-3.6495e-05,  2.1445e-06,  7.4410e-06, -4.3725e-07, -3.4228e-05,\n",
       "            2.0113e-06,  6.3381e-06, -3.7244e-07,  2.1907e-05, -1.2873e-06,\n",
       "           -4.3957e-05,  2.5830e-06, -2.8295e-05,  1.6627e-06, -3.8517e-05,\n",
       "            2.2634e-06, -5.0373e-06,  2.9600e-07]]),\n",
       "  'grad_wrt_output': tensor([[ 0.1019, -0.0208,  0.0956, -0.0177, -0.0612,  0.1228,  0.0790,  0.1076,\n",
       "            0.0141]]),\n",
       "  'input': tensor([[ 0.6582, -0.0387]]),\n",
       "  'scalings': tensor([[0.0000e+00, 2.8126e-16],\n",
       "          [0.0000e+00, 4.0636e-17],\n",
       "          [0.0000e+00, 1.8806e-17],\n",
       "          [0.0000e+00, 2.8507e-17],\n",
       "          [0.0000e+00, 8.3426e-17],\n",
       "          [0.0000e+00, 2.7343e-16],\n",
       "          [0.0000e+00, 3.3934e-18],\n",
       "          [0.0000e+00, 7.8917e-16],\n",
       "          [0.0000e+00, 2.4817e-02]])},\n",
       " Linear(in_features=9, out_features=1, bias=False): {'UA': tensor([[-0.0829, -0.3697,  0.2758, -0.8091,  0.1063, -0.0862, -0.1170, -0.0256,\n",
       "            0.3043],\n",
       "          [ 0.2666,  0.4445,  0.1717, -0.0114, -0.3387, -0.6407,  0.0511,  0.1350,\n",
       "            0.3946],\n",
       "          [-0.0113, -0.3914, -0.2586,  0.1598,  0.1469, -0.1274,  0.7542,  0.0090,\n",
       "            0.3841],\n",
       "          [-0.0558,  0.0276,  0.5338,  0.1137, -0.4975,  0.5722,  0.2340, -0.0308,\n",
       "            0.2600],\n",
       "          [ 0.1250,  0.3864, -0.6520, -0.3700, -0.1510,  0.4109,  0.0107,  0.0129,\n",
       "            0.2850],\n",
       "          [-0.1380,  0.4738,  0.2809,  0.0820,  0.7347,  0.1651,  0.0469,  0.0526,\n",
       "            0.3142],\n",
       "          [ 0.5739, -0.3589, -0.0283,  0.2999,  0.1238,  0.1653, -0.4501,  0.2265,\n",
       "            0.3931],\n",
       "          [-0.7410, -0.0941, -0.1829,  0.2130, -0.1629, -0.0865, -0.3314,  0.3141,\n",
       "            0.3474],\n",
       "          [-0.0756, -0.0174, -0.0661,  0.1666, -0.0200, -0.0866, -0.2132, -0.9095,\n",
       "            0.2852]]),\n",
       "  'UB': tensor([[1.]]),\n",
       "  'aproximate_Fisher': tensor([[0.4715, 0.6116, 0.5952, 0.4029, 0.4417, 0.4870, 0.6093, 0.5384, 0.4419],\n",
       "          [0.6116, 0.7932, 0.7720, 0.5225, 0.5728, 0.6316, 0.7902, 0.6983, 0.5732],\n",
       "          [0.5952, 0.7720, 0.7514, 0.5086, 0.5575, 0.6147, 0.7691, 0.6797, 0.5579],\n",
       "          [0.4029, 0.5225, 0.5086, 0.3442, 0.3774, 0.4161, 0.5206, 0.4601, 0.3776],\n",
       "          [0.4417, 0.5728, 0.5575, 0.3774, 0.4137, 0.4562, 0.5707, 0.5043, 0.4140],\n",
       "          [0.4870, 0.6316, 0.6147, 0.4161, 0.4562, 0.5030, 0.6292, 0.5561, 0.4564],\n",
       "          [0.6093, 0.7902, 0.7691, 0.5206, 0.5707, 0.6292, 0.7872, 0.6957, 0.5710],\n",
       "          [0.5384, 0.6983, 0.6797, 0.4601, 0.5043, 0.5561, 0.6957, 0.6148, 0.5046],\n",
       "          [0.4419, 0.5732, 0.5579, 0.3776, 0.4140, 0.4564, 0.5710, 0.5046, 0.4142]]),\n",
       "  'empirical_fisher': tensor([[0.4715, 0.6116, 0.5952, 0.4029, 0.4417, 0.4870, 0.6093, 0.5384, 0.4419],\n",
       "          [0.6116, 0.7932, 0.7720, 0.5225, 0.5728, 0.6316, 0.7902, 0.6983, 0.5732],\n",
       "          [0.5952, 0.7720, 0.7514, 0.5086, 0.5575, 0.6147, 0.7691, 0.6797, 0.5579],\n",
       "          [0.4029, 0.5225, 0.5086, 0.3442, 0.3774, 0.4161, 0.5206, 0.4601, 0.3776],\n",
       "          [0.4417, 0.5728, 0.5575, 0.3774, 0.4137, 0.4562, 0.5707, 0.5043, 0.4140],\n",
       "          [0.4870, 0.6316, 0.6147, 0.4161, 0.4562, 0.5030, 0.6292, 0.5561, 0.4564],\n",
       "          [0.6093, 0.7902, 0.7691, 0.5206, 0.5707, 0.6292, 0.7872, 0.6957, 0.5710],\n",
       "          [0.5384, 0.6983, 0.6797, 0.4601, 0.5043, 0.5561, 0.6957, 0.6148, 0.5046],\n",
       "          [0.4419, 0.5732, 0.5579, 0.3776, 0.4140, 0.4564, 0.5710, 0.5046, 0.4142]]),\n",
       "  'grad_wrt_output': tensor([[-1.5102]]),\n",
       "  'input': tensor([[0.4547, 0.5897, 0.5740, 0.3885, 0.4259, 0.4696, 0.5875, 0.5192, 0.4261]],\n",
       "         grad_fn=<SigmoidBackward>),\n",
       "  'scalings': tensor([[3.4469e-14, 1.2364e-15, 2.6620e-14, 5.0644e-16, 7.9131e-16, 4.5579e-15,\n",
       "           3.6590e-14, 1.8232e-14, 5.0932e+00]])}}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EKFAC_one.stored_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It seems like when I only have a single entry in the mini-batch, the last layer's approximate FIsher is equal to the \n",
    "# exact Fisher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

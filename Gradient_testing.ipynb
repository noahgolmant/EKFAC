{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import EKFAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing gradient passed to the backward hook function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EKFAC code, we register a backwards hook for every **linear** layer that gets called every time ``loss.backward()`` is called. One of the arguments passed to the hook (which our hook saves) is the gradient of the loss function with respect to the **output** of the linear layer.  In this notebook, we verify that that argument matches analytic calculations, for some simple networks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-layer linear network with a single output, and no bias\n",
    "\n",
    "The first network we test is simply a linear network with an arbitrary input dimension but a one-dimensional output, i.e. taking the dot product of the input with a weights vector.  \n",
    "\n",
    "That is, \n",
    "\\begin{equation}\n",
    "y = \\vec{W}\\cdot\\vec{x}\n",
    "\\end{equation}\n",
    "\n",
    "We will use two different loss functions, either the mean-squared loss or the total squared loss, where the mean or sum respectively is taken over all of the mini-batch inputs.  So if the data in a mini-batch is denoted $\\{ ( \\vec{x}_i, y_i ) \\}$, then the MSE loss is given by \n",
    "\\begin{equation}\n",
    "L_{MSE} = \\frac{1}{N_b}\\sum_i (y_i - \\vec{W} \\cdot \\vec{x})^2 = \\frac{1}{N_b}\\sum_i (y_i - y^{mod}_i)^2\n",
    "\\end{equation}\n",
    "and the total squared loss is given by \n",
    "\\begin{equation}\n",
    "L_{TSE} = \\sum_i (y_i - \\vec{W} \\cdot \\vec{x})^2 = \\sum_i (y_i - y^{mod}_i)^2,\n",
    "\\end{equation}\n",
    "where $y^{mod}_i$ is defined via $y^{mod}_i = y(\\vec{x}_i)$.  \n",
    "\n",
    "Then, when we take gradients of the loss function with respect to the outputs $y^{mod}_i$ and get, for each loss function, that\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_{MSE}}{\\partial y^{mod}_i} = -\\frac{2}{N_b} (y_i - y^{mod}_i) \n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_{TSE}}{\\partial y^{mod}_i} = -2 (y_i - y^{mod}_i) \n",
    "\\end{equation}\n",
    "\n",
    "The only difference between the two is the factor of $N_b$, which comes from the definitions of the $TSE$ and $MSE$ functions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = 10\n",
    "D_in = 2\n",
    "D_out = 1\n",
    "\n",
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_out, bias=False),\n",
    ")\n",
    "\n",
    "W = list(linear_model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EKFAC_lin = EKFAC.EKFAC(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(Nbatch, D_in)\n",
    "y = torch.randn(Nbatch, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = linear_model(x)\n",
    "\n",
    "loss_functions = {'MSE': torch.nn.MSELoss(reduction='mean'),\n",
    "          'TSE': torch.nn.MSELoss(reduction='sum')}\n",
    "\n",
    "loss_gradients  = {'MSE': lambda y, y_mod, N_batch: -2/N_batch*(y-y_mod),\n",
    "                   'TSE': lambda y, y_mod, N_batch: -2*(y-y_mod)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'TSE'\n",
    "\n",
    "loss_fun = loss_functions[loss_type]\n",
    "loss_grad = loss_gradients[loss_type]\n",
    "\n",
    "l = loss_fun(y, y_mod)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the stored items in the ``EKFAC`` object to the gradients we calculated analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_passed_to_hook = EKFAC_lin.stored_items[list(EKFAC_lin.stored_items.keys())[0]]['grad_wrt_output']\n",
    "gradient_analytic = loss_grad(y, y_mod, Nbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.7901],\n",
       "        [-14.3245],\n",
       "        [-15.9768],\n",
       "        [ 17.5171],\n",
       "        [-16.2203],\n",
       "        [ 10.7378],\n",
       "        [ -6.6827],\n",
       "        [  9.1003],\n",
       "        [ 10.4937],\n",
       "        [ -0.4674]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_passed_to_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0790],\n",
       "        [-1.4325],\n",
       "        [-1.5977],\n",
       "        [ 1.7517],\n",
       "        [-1.6220],\n",
       "        [ 1.0738],\n",
       "        [-0.6683],\n",
       "        [ 0.9100],\n",
       "        [ 1.0494],\n",
       "        [-0.0467]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import EKFAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing gradient passed to the backward hook function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EKFAC code, we register a backwards hook for every **linear** layer that gets called every time ``loss.backward()`` is called. One of the arguments passed to the hook (which our hook saves) is the gradient of the loss function with respect to the **output** of the linear layer.  In this notebook, we verify that that argument matches analytic calculations, for some simple networks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-layer linear network with a single output, and no bias\n",
    "\n",
    "The first network we test is simply a linear network with an arbitrary input dimension but a one-dimensional output, i.e. taking the dot product of the input with a weights vector.  \n",
    "\n",
    "That is, \n",
    "\\begin{equation}\n",
    "y = \\vec{W}\\cdot\\vec{x}\n",
    "\\end{equation}\n",
    "\n",
    "We will use two different loss functions, either the mean-squared loss or the total squared loss, where the mean or sum respectively is taken over all of the mini-batch inputs.  So if the data in a mini-batch is denoted $\\{ ( \\vec{x}_i, y_i ) \\}$, then the MSE loss is given by \n",
    "\\begin{equation}\n",
    "L_{MSE} = \\frac{1}{N_b}\\sum_i (y_i - \\vec{W} \\cdot \\vec{x})^2 = \\frac{1}{N_b}\\sum_i (y_i - y^{mod}_i)^2\n",
    "\\end{equation}\n",
    "and the total squared loss is given by \n",
    "\\begin{equation}\n",
    "L_{TSE} = \\sum_i (y_i - \\vec{W} \\cdot \\vec{x})^2 = \\sum_i (y_i - y^{mod}_i)^2,\n",
    "\\end{equation}\n",
    "where $y^{mod}_i$ is defined via $y^{mod}_i = y(\\vec{x}_i)$.  \n",
    "\n",
    "Then, when we take gradients of the loss function with respect to the outputs $y^{mod}_i$ and get, for each loss function, that\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_{MSE}}{\\partial y^{mod}_i} = -\\frac{2}{N_b} (y_i - y^{mod}_i) \n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_{TSE}}{\\partial y^{mod}_i} = -2 (y_i - y^{mod}_i) \n",
    "\\end{equation}\n",
    "\n",
    "The only difference between the two is the factor of $N_b$, which comes from the definitions of the $TSE$ and $MSE$ functions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = 10\n",
    "D_in = 2\n",
    "D_out = 1\n",
    "\n",
    "loss_type = 'TSE'\n",
    "\n",
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_out, bias=False),\n",
    ")\n",
    "\n",
    "W = list(linear_model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "EKFAC_lin = EKFAC.EKFAC(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(Nbatch, D_in)\n",
    "y = torch.randn(Nbatch, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = linear_model(x)\n",
    "\n",
    "loss_functions = {'MSE': torch.nn.MSELoss(reduction='mean'),\n",
    "          'TSE': torch.nn.MSELoss(reduction='sum')}\n",
    "\n",
    "loss_gradients  = {'MSE': lambda y, y_mod, N_batch: -2/N_batch*(y-y_mod),\n",
    "                   'TSE': lambda y, y_mod, N_batch: -2*(y-y_mod)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = loss_functions[loss_type]\n",
    "loss_grad = loss_gradients[loss_type]\n",
    "\n",
    "l = loss_fun(y, y_mod)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the stored items in the ``EKFAC`` object to the gradients we calculated analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gradient_saved_by_hook = EKFAC_lin.stored_items[list(EKFAC_lin.stored_items.keys())[0]]['grad_wrt_output']\n",
    "    gradient_analytic = loss_grad(y, y_mod, Nbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient saved by hook: tensor([[-3.6643e+01, -1.3093e+01, -4.2351e+01,  1.6123e-02, -5.3818e+00,\n",
      "          9.6941e+00, -3.9953e+00,  2.3490e+01, -5.5934e+00, -2.1213e+01]])\n",
      "Analytically calculated gradient: tensor([[-3.6643e+00, -1.3093e+00, -4.2351e+00,  1.6123e-03, -5.3818e-01,\n",
      "          9.6941e-01, -3.9953e-01,  2.3490e+00, -5.5934e-01, -2.1213e+00]])\n"
     ]
    }
   ],
   "source": [
    "print('Gradient saved by hook: {}'.format(gradient_saved_by_hook.t()))\n",
    "print('Analytically calculated gradient: {}'.format(gradient_analytic.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the ``gradient_analytic`` is a factor of 10 smaller the ``gradient_saved_by_hook``, which is the batch_size.  This is because we're multiplying the ``gradient_passed_to_hook`` by ``batch_size`` before saving the gradient.  We can see that this is true regardless of whether we use MSE or TSE.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the EKFAC paper, we know that if we define, for a linear layer, $h$ as the input to that layer, and $\\delta$ as the derivative of the loss function with respect to that layer, then the gradient of the loss function with respect to the weight parameter of the layer is given by $h\\delta^T$.  Let's see if that holds true currently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a single linear layer, $h$ is given by $x$, the input to the network, and $\\delta$ is given by the analytically calculated gradient above.  To average over the mini-batch, we divide finally by the input size, ``x.size(0)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weight_grad = x.t() @ gradient_saved_by_hook / x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad: tensor([[10.7965, -1.1092]])\n",
      "test W grad: tensor([[10.7965],\n",
      "        [-1.1092]])\n"
     ]
    }
   ],
   "source": [
    "print('W.grad: {}'.format(W.grad))\n",
    "print('test W grad: {}'.format(test_weight_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see why we had to multiply by the batch size earlier, that is, why we needed the ``gradient_saved_by_hook`` to be a factor of ``batch_size`` larger than the true gradient of the loss function.  If we didn't do this, we'd get the wrong answer when we averaged $h\\delta^T$.  The reason is the following: what's stored in ``gradient_passed_to_hook`` is the gradient of the total loss function, $L$, with respect to the parameters.  But when we pass a vector with multiple inputs, i.e., a mini-batch, then what we actually want to store is the gradient of the function which is being averaged over to get $L$.  So if we write $L = \\frac{1}{N_B} \\sum_i L_i$, then we actually want to store the gradient of $L_i$, not the gradient of $L$.  This is why we multiply by $N_B$ before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
